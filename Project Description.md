# Fine-Tune-BERT-for-Text-Classification-with-TensorFlow
This project is about fine-tuning a Bidirectional Transformers for Language Understanding (BERT) model for text classification with TensorFlow. This project is the comprises of preprocess and tokenization of data for BERT classification, it also includes building TensorFlow input pipelines for text data with the tf.data API, and training and evaluating a fine-tuned BERT model for text classification with TensorFlow 2 and TensorFlow Hub.

# Steps to complete the project
These are the following steps that I followed to complete the project.

Step 1: Setting TensorFlow and Colab Runtime

Step 2: Loading the Quora Insincere Questions Dataset (Link of the dataset is given in the colab notebook)

Step 3: Creating tf.data.Datasets for Training and Evaluation

Step 4: Downloading a Pre-trained BERT Model from TensorFlow Hub

Step 5: Tokenizening and Preprocesing Text for BERT

Step 6: Wrapping a Python Function into a TensorFlow op for Eager Execution

Step 7: Creating a TensorFlow Input Pipeline with tf.data

Step 8: Adding a Classification Head to the BERT hub.KerasLayer

Step 9: Fine-Tune BERT for Text Classification

Step 10: Evaluate the BERT Text Classification Mode
